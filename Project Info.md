Project Topic Name: Resource efficient knowledge distillation from multiple domains in time series forecasting



Description of the Project:

1. The task is to develop a resource-efficient knowledge distillation technique which will distill the knowledge from the complex teacher models (which have been trained on time-series data from multiple domains).
   The aim is to transfer knowledge from these large, complex teacher models (trained on multi-domain datasets) to smaller student model that maintain high forecasting accuracy with reduced computational costs.
2. Team must collect at least 4-5 multi-domain time series datasets (e.g., finance, healthcare, climate) and get them verified by TAs.
3. Team should explore the robust pre-trained models suitable for time series forecasting and verify the selected modelâ€™s appropriateness with TAs.
4. Team will design a knowledge distillation framework to transfer knowledge from the teacher model to a smaller student model. This must be verified by the concerned TAs.
5. A suitable loss function for the knowledge distillation task must be designed, and the student model will be trained using this loss function. This must be verified by the concerned TAs.
6. Team must explore suitable performance metrics for evaluating forecasting accuracy and resource efficiency and confirm these metrics with TAs.
7. The team will showcase training logs, performance metrics, and visualizations for all the datasets used.



Deliverables(during Intensive Assessment):

1. Trained/finetuned model: All models (teacher and student), along with code files and documentation.
2. An interface where the user can input multi-variate time series data in CSV format and visualize the data in graphical form (e.g., line plots).
3. The interface should display forecast results alongside the original time series data. Users should be able to view the predictions and actual values together with a clear distinction. This should be done for both
   teacher and the distilled model.
4. Display of performance metrics, including forecasting accuracy and resource efficiency (e.g., model size, inference time, memory usage). This should also be done for both teacher and the distilled model.
5. Stage-wise report: Detailed report covering project methodology, experiments, dataset details, and interface development.
6. Team needs to complete the task by the intensive assessment.



Topics for Preparatory Presentation: CNN, RNN, Transformer, Vision Transformers, Knowledge Distillation



[Project Timeline Sheet](https://docs.google.com/spreadsheets/d/1SGf3IMUTBB6rXv-JBKukWJrC4l7nFNbNQrywF4Wj0kc/edit?gid=0#gid=0)



[Pre-prep instructions](https://docs.google.com/document/d/1pI1HIOBO4NgIQXwVR8T05nLJSwKDcJd3syhiiQUX6yo/edit?tab=t.0)

