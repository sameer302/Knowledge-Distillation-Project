1) Knowledge distillation in deep learning and its applications


2) Compression

base-level classifiers: Base-level classifiers are the individual, simple models (like a single decision tree or neural network) that make up an ensemble. The ensemble combines their predictions to achieve better overall performance than any single model.

Large ensembles are powerful but slow and memory-heavy. Theyâ€™re impractical for huge datasets (like Google queries) or small devices (like PDAs). The paper proposes compressing ensembles into smaller, faster models without much loss in performance.

